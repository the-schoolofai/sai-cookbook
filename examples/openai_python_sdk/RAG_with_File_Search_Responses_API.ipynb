{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e84c00a",
   "metadata": {},
   "source": [
    "# Doing RAG on PDFs using File Search in the Responses API\n",
    "\n",
    "Although RAG can be overwhelming, searching amongst PDF file shouldn't be complicated. One of the most adopted options as of now is parsing your PDF, defining your chunking strategies, uploading those chunks to a storage provider, running embeddings on those chunks of texts and storing those embeddings in a vector database. And that's only the setup — retrieving content in LLM workflow also requires multiple steps.\n",
    "\n",
    "This is where file search — a hosted tool you can use in the Responses API — comes in. It allows you to search your knowledge base and generate an answer based on the retrieved content. In this cookbook, we'll upload those PDFs to a vector store on OpenAI and use file search to fetch additional context from this vector store to answer the questions we generated in the first step. Then, we'll initially create a small set of questions based on PDFs extracted from OpenAI's blog.\n",
    "\n",
    "_File search was previously available on the Assistants API. It's now available on the new Responses API, an API that can be stateful or stateless, and with from new features like metadata filtering_\n",
    "\n",
    "\n",
    "### Traditional RAG approach\n",
    "\n",
    "One of the most adopted options involves:\n",
    "\n",
    "#### Setup Steps\n",
    "1. Parse PDF - Extract text from PDF documents\n",
    "2. Chunk Text - Define and apply chunking strategies\n",
    "3. Upload Chunks - Upload chunks to storage provider\n",
    "4. Generate Embeddings - Run embeddings on text chunks\n",
    "5. Store in Vector DB - Store embeddings in vector database\n",
    "\n",
    "#### Retrieval Steps\n",
    "1. Query Embeddings - Convert user query to embeddings\n",
    "2. Retrieve Matches - Find similar chunks in vector DB\n",
    "3. Pass to LLM - Send context to language model\n",
    "\n",
    "### File Search approach (Responses API)\n",
    "\n",
    "A hosted tool that simplifies the entire process:\n",
    "\n",
    "#### Setup Steps\n",
    "1. Upload PDFs - Upload documents directly to OpenAI storage files\n",
    "2. Create Vector Store - Automatic vector store creation\n",
    "3. Enable File Search - Activate the tool in API\n",
    "\n",
    "#### Query Steps\n",
    "1. Generate Questions - Create questions from content\n",
    "2. Search Vector Store - Query the vector store\n",
    "3. Retrieve Context - Get relevant information\n",
    "4. Generate Answer - LLM generates response\n",
    "\n",
    "#### Key Features\n",
    "- Stateful/Stateless modes\n",
    "- Metadata filtering\n",
    "- Previously available on Assistants API\n",
    "- Now available on new Responses API\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| **Aspect** | **Traditional RAG** | **File Search (Responses API)** |\n",
    "|------------|---------------------|----------------------------------|\n",
    "| **PDF Parsing** | Manual | Automated |\n",
    "| **Chunking** | Define strategy yourself | Handled automatically |\n",
    "| **Storage** | Upload to storage provider | Upload directly to OpenAI storage files |\n",
    "| **Embeddings** | Generate & manage yourself | Automatically generated |\n",
    "| **Vector DB** | Set up separately | Built-in vector stores |\n",
    "| **Retrieval** | Multi-step implementation | Single API call |\n",
    "| **State Management** | Custom implementation | Stateful/Stateless options |\n",
    "| **Filtering** | Build yourself | Metadata filtering built-in |\n",
    "| **Complexity** | High | Low |\n",
    "| **Setup Time** | Long | Short |\n",
    "| **Maintenance** | Ongoing management required | Fully managed by OpenAI |\n",
    "| **Cost** | Infrastructure + Development | API usage only |\n",
    "\n",
    "\n",
    "## Creating Vector Store with PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883e38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf tqdm openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c11fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pypdf==6.1.3\n",
    "# tqdm==4.67.1\n",
    "# openai==2.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d086c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pypdf\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "dir_pdfs = 'openai_blog_pdfs' # have those PDFs stored locally here\n",
    "pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e05f59",
   "metadata": {},
   "source": [
    "We will create a Vector Store on OpenAI API and upload PDFs to the Vector Store. OpenAI will read those PDFs, separate the content into multiple chunks of text, run embeddings on those and store those embeddings and the text in the Vector Store. It will enable us to query this Vector Store to return relevant content based on a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961fbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_single_pdf(file_path: str, vector_store_id: str):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n",
    "        attach_response = client.vector_stores.files.create(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=file_response.id\n",
    "        )\n",
    "        return {\"file\": file_name, \"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {file_name}: {str(e)}\")\n",
    "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "def upload_pdf_files_to_vector_store(vector_store_id: str):\n",
    "    pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n",
    "    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n",
    "    \n",
    "    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n",
    "            result = future.result()\n",
    "            if result[\"status\"] == \"success\":\n",
    "                stats[\"successful_uploads\"] += 1\n",
    "            else:\n",
    "                stats[\"failed_uploads\"] += 1\n",
    "                stats[\"errors\"].append(result)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def create_vector_store(store_name: str) -> dict:\n",
    "    try:\n",
    "        vector_store = client.vector_stores.create(name=store_name)\n",
    "        details = {\n",
    "            \"id\": vector_store.id,\n",
    "            \"name\": vector_store.name,\n",
    "            \"created_at\": vector_store.created_at,\n",
    "            \"file_count\": vector_store.file_counts.completed\n",
    "        }\n",
    "        print(\"Vector store created:\", details)\n",
    "        return details\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdd2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created: {'id': 'vs_690b37e7f014819191fe305eca6e0eff', 'name': 'openai_blog_store', 'created_at': 1762342888, 'file_count': 0}\n",
      "2 PDF files to process. Uploading in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.52s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_files': 2, 'successful_uploads': 2, 'failed_uploads': 0, 'errors': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_name = \"openai_blog_store\"\n",
    "vector_store_details = create_vector_store(store_name)\n",
    "upload_pdf_files_to_vector_store(vector_store_details[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913eabb",
   "metadata": {},
   "source": [
    "## Standalone vector search\n",
    "\n",
    "Now that vector store is ready, we are able to query the Vector Store directly and retrieve relevant content for a specific query. Using the new [vector search API](https://platform.openai.com/docs/api-reference/vector-stores/search), we're able to find relevant items from our knowledge base without necessarily integrating it in an LLM query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3bc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's Deep Research?\"\n",
    "search_results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store_details['id'],\n",
    "    query=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892495aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.979164968744516\n",
      "3601 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9336562593621925\n",
      "3584 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.926245567222169\n",
      "2646 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.9240999630209855\n",
      "2969 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.8671716166920782\n",
      "3644 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.8220330216618142\n",
      "3068 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.7722818472064626\n",
      "3350 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.7502893980744584\n",
      "3021 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.6897558361663177\n",
      "3344 of character of content from Introducing deep research _ OpenAI.pdf with a relevant score of 0.6731493959010373\n"
     ]
    }
   ],
   "source": [
    "for result in search_results.data:\n",
    "    print(str(len(result.content[0].text)) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str(result.score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8454521",
   "metadata": {},
   "source": [
    "We can see that different size (and under-the-hood different texts) have been returned from the search query. They all have different relevancy score that are calculated by ranker which uses hybrid search.\n",
    "\n",
    "## Integrating search results with LLM in a single API call\n",
    "\n",
    "However instead of querying the vector store and then passing the data into the Responses or Chat Completion API call, an even more convenient way to use this search results in an LLM query would be to plug use file_search tool as part of OpenAI Responses API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74715b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files used: {'Introducing deep research _ OpenAI.pdf'}\n",
      "Response:\n",
      "**Deep Research** is an advanced capability introduced by OpenAI in ChatGPT, designed to conduct sophisticated, multi-step research tasks independently. It synthesizes large volumes of information from online sources and leverages advanced reasoning to produce comprehensive reports, akin to the work of a research analyst.\n",
      "\n",
      "### Key Features:\n",
      "1. **Multi-step Research**: It efficiently handles complex queries that would otherwise take significant time for a human to research.\n",
      "2. **Information Synthesis**: Deep research analyzes and consolidates insights from various online resources, providing detailed, documented outputs with clear citations.\n",
      "3. **Use Cases**: It is particularly useful for professionals in fields like finance, science, and engineering, as well as for consumers seeking in-depth information for major purchases.\n",
      "4. **Performance Metrics**: The capability is trained using end-to-end reinforcement learning, achieving notable accuracy in expert-level evaluations.\n",
      "\n",
      "### Accessibility and Future Enhancements:\n",
      "Deep Research is initially available to Pro users, with plans to expand access. Future iterations will enhance features like embedding visuals and improving interfaces for better user experiences. \n",
      "\n",
      "To optimize the experience, users simply enter a query specifying their research needs, and the AI generates a report that summarizes the steps taken and the sources consulted.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's Deep Research?\"\n",
    "response = client.responses.create(\n",
    "    input= query,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store_details['id']],\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Extract annotations from the response\n",
    "annotations = response.output[1].content[0].annotations\n",
    "    \n",
    "# Get top-k retrieved filenames\n",
    "retrieved_files = set([result.filename for result in annotations])\n",
    "\n",
    "print(f'Files used: {retrieved_files}')\n",
    "print('Response:')\n",
    "print(response.output[1].content[0].text) # 0 being the filesearch call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5f38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e42d60c3",
   "metadata": {},
   "source": [
    "We can see that `gpt-4o-mini` was able to answer a query that required more recent, specialised knowledge about OpenAI's Deep Research. It used content from the file `Introducing deep research _ OpenAI.pdf` that had chunks of texts that were the most relevant. If we want to go even deeper in the analysis of chunk of text retrieved, we can also analyse the different texts that were returned by the search engine by adding `include=[\"output[*].file_search_call.search_results\"]` to query.\n",
    "\n",
    "## Evaluating performance\n",
    "\n",
    "What is key for those information retrieval system is to also measure the relevance & quality of files retrieved for those answers. The following steps of this cookbook will consist in generating an evaluation dataset and calculating different metrics over this generated dataset. This is an imperfect approach and we'll always recommend to have a human-verified evaluation dataset for your own use-cases, but it will show you the methodology to evaluate those.  It will be imperfect because some of the questions generated might be generic (e.g: What's said by the main stakeholder in this document) and this retrieval test will have a hard time to figure out which document that question was generated for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00459723",
   "metadata": {},
   "source": [
    "### Generating evaluations\n",
    "\n",
    "We will create functions that will read through the PDFs we have locally and generate a question that can only be answered by this document. Therefore it'll create evaluation dataset that we can use after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a90a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            reader = pypdf.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def generate_questions(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    prompt = (\n",
    "        \"Can you generate a question that can only be answered from this document?:\\n\"\n",
    "        f\"{text}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    response = client.responses.create(\n",
    "        input=prompt,\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "\n",
    "    question = response.output[0].content[0].text\n",
    "\n",
    "    return question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d340e",
   "metadata": {},
   "source": [
    "If we run the function generate_question for the first PDF file we will be able to see the kind of question it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd83911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 172 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 178 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 190 0 (offset 0)\n",
      "Ignoring wrong pointing object 191 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is Operator and when was it released to Pro users in the U.S.?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_questions(pdf_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52282cd",
   "metadata": {},
   "source": [
    "We can now generate all the questions for all the PDFs we've got stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0448b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 46 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 88 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 94 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 100 0 (offset 0)\n",
      "Ignoring wrong pointing object 101 0 (offset 0)\n",
      "Ignoring wrong pointing object 166 0 (offset 0)\n",
      "Ignoring wrong pointing object 167 0 (offset 0)\n",
      "Ignoring wrong pointing object 172 0 (offset 0)\n",
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 178 0 (offset 0)\n",
      "Ignoring wrong pointing object 179 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n",
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 190 0 (offset 0)\n",
      "Ignoring wrong pointing object 191 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 131 0 (offset 0)\n",
      "Ignoring wrong pointing object 132 0 (offset 0)\n",
      "Ignoring wrong pointing object 328 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# Generate questions for each PDF and store in a dictionary\n",
    "questions_dict = {}\n",
    "for pdf_path in pdf_files:\n",
    "    questions = generate_questions(pdf_path)\n",
    "    questions_dict[os.path.basename(pdf_path)] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4cd6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Introducing Operator _ OpenAI.pdf': \"What is the primary function of OpenAI's Operator as mentioned in this document?\",\n",
       " 'Introducing deep research _ OpenAI.pdf': 'What specific capabilities and benefits does the \"deep research\" feature in ChatGPT offer for users conducting intensive knowledge work?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71faa72",
   "metadata": {},
   "source": [
    "We now have a dictionary of `filename:question` that we can loop through and ask gpt-4o(-mini) about without providing the document, and gpt-4o should be able to find the relevant document in the Vector Store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b752fd",
   "metadata": {},
   "source": [
    "### Evaluating\n",
    "\n",
    "We'll convert dictionary into a dataframe and process it using gpt-4o-mini. We will look out for the expected file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4486acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for filename, query in questions_dict.items():\n",
    "    rows.append({\"query\": query, \"_id\": filename.replace(\".pdf\", \"\")})\n",
    "\n",
    "# Metrics evaluation parameters\n",
    "k = 5\n",
    "total_queries = len(rows)\n",
    "correct_retrievals_at_k = 0\n",
    "reciprocal_ranks = []\n",
    "average_precisions = []\n",
    "\n",
    "def process_query(row):\n",
    "    query = row['query']\n",
    "    expected_filename = row['_id'] + '.pdf'\n",
    "    # Call file_search via Responses API\n",
    "    response = client.responses.create(\n",
    "        input=query,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=[{\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store_details['id']],\n",
    "            \"max_num_results\": k,\n",
    "        }],\n",
    "        tool_choice=\"required\" # it will force the file_search, while not necessary, it's better to enforce it as this is what we're testing\n",
    "    )\n",
    "    # Extract annotations from the response\n",
    "    annotations = None\n",
    "    if hasattr(response.output[1], 'content') and response.output[1].content:\n",
    "        annotations = response.output[1].content[0].annotations\n",
    "    elif hasattr(response.output[1], 'annotations'):\n",
    "        annotations = response.output[1].annotations\n",
    "\n",
    "    if annotations is None:\n",
    "        print(f\"No annotations for query: {query}\")\n",
    "        return False, 0, 0\n",
    "\n",
    "    # Get top-k retrieved filenames\n",
    "    retrieved_files = [result.filename for result in annotations[:k]]\n",
    "    if expected_filename in retrieved_files:\n",
    "        rank = retrieved_files.index(expected_filename) + 1\n",
    "        rr = 1 / rank\n",
    "        correct = True\n",
    "    else:\n",
    "        rr = 0\n",
    "        correct = False\n",
    "\n",
    "    # Calculate Average Precision\n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    for i, fname in enumerate(retrieved_files):\n",
    "        if fname == expected_filename:\n",
    "            num_relevant += 1\n",
    "            precisions.append(num_relevant / (i + 1))\n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    \n",
    "    if expected_filename not in retrieved_files:\n",
    "        print(\"Expected file NOT found in the retrieved files!\")\n",
    "        \n",
    "    if retrieved_files and retrieved_files[0] != expected_filename:\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Expected file: {expected_filename}\")\n",
    "        print(f\"First retrieved file: {retrieved_files[0]}\")\n",
    "        print(f\"Retrieved files: {retrieved_files}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    \n",
    "    return correct, rr, avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d3d1ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 1.0, 1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query(rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f2646",
   "metadata": {},
   "source": [
    "Recall & Precision are at 1 for this example, and file ranked first so we're having a MRR and MAP = 1 on this example.\n",
    "\n",
    "We can now execute this processing on set of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18fd9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.08s/it]\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_query, rows), total=total_queries))\n",
    "\n",
    "correct_retrievals_at_k = 0\n",
    "reciprocal_ranks = []\n",
    "average_precisions = []\n",
    "\n",
    "for correct, rr, avg_precision in results:\n",
    "    if correct:\n",
    "        correct_retrievals_at_k += 1\n",
    "    reciprocal_ranks.append(rr)\n",
    "    average_precisions.append(avg_precision)\n",
    "\n",
    "recall_at_k = correct_retrievals_at_k / total_queries\n",
    "precision_at_k = recall_at_k  # In this context, same as recall\n",
    "mrr = sum(reciprocal_ranks) / total_queries\n",
    "map_score = sum(average_precisions) / total_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce3b74",
   "metadata": {},
   "source": [
    "The outputs logged above would either show that a file wasn't ranked first when evaluation dataset expected it to rank first or that it wasn't found at all. As we can see from imperfect evaluation dataset, some questions were generic and expected another doc, which this retrieval system didn't specifically retrieved for this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fa467fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at k=5:\n",
      "Recall@5: 1.0000\n",
      "Precision@5: 1.0000\n",
      "Mean Reciprocal Rank (MRR): 1.0000\n",
      "Mean Average Precision (MAP): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics with k\n",
    "print(f\"Metrics at k={k}:\")\n",
    "print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
    "print(f\"Precision@{k}: {precision_at_k:.4f}\")\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ccf41",
   "metadata": {},
   "source": [
    "With this cookbook we were able to see how to:\n",
    "- Generate a dataset of evaluations using PDF context-stuffing (leveraging vision modality of 4o) and traditional PDF readers\n",
    "- Create a vector store and populate it with PDF\n",
    "- Get an LLM answer to a query, leveraging a RAG system available out-of-the-box with `file_search` tool call in OpenAI's Response API\n",
    "- Understand how chunks of texts are retrieved, ranked and used as part of the Response API\n",
    "- Measure accuracy, precision, retrieval, MRR and MAP on the dataset of evaluations previously generated\n",
    "\n",
    "By using file search with Responses, you can simplify RAG architecture and leverage this in a single API call using the new Responses API. File storage, embeddings, retrieval all integrated in one tool!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sai-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
